---
title: "Natural Language Processing"
---

Natural Language Processing (NLP) is a field of computer science that focuses on enabling computers to understand, interpret, and generate human language. NLP methods combine linguistics, statistics, and machine learning to analyze text and speech data, enabling tasks such as text classification, sentiment analysis, machine translation, and question answering. Modern NLP often relies on deep learning models, such as transformers, which can capture complex patterns in language and context. By bridging human communication and computational systems, NLP supports applications in information retrieval, virtual assistants, and automated content generation.

- [Introduction to NLP - Stanford CS224N - YouTube Playlist](https://www.youtube.com/watch?v=rmVRLeJRkl4&list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ)

::: {.callout-note icon=false}
## Further Keywords
human language, word meaning, word2vec algorithm, word2vec objective function gradients, optimization basics, word vectors, PyTorch, dependency parsing, machine translation, question answering, recurrent networks, attention, transformers, distributional semantics, chain rule
:::

```{r tag_IntroNLP, results = 'asis', echo = FALSE, warning = FALSE, message = FALSE}
source("../utils.R")
render_tags("IntroNLP")
```

Deep Learning has become a cornerstone of modern NLP, enabling significant advancements in the field. Deep learning models have demonstrated remarkable capabilities in understanding and generating human language. Techniques such as word embeddings, recurrent neural networks (RNNs), and transformers have revolutionized NLP tasks by allowing models to capture semantic relationships, context, and long-range dependencies in text data.

- [Deep Learning for NLP (DL4NLP) - Page](https://slds-lmu.github.io/dl4nlp/)

::: {.callout-note icon=false}
## Further Keywords
machine learning basics, NLP tasks, neural probabilistic language models, word embeddings, deep learning basics, attention, ELMo, tokenization, transformer, encoder, decoder, long sequences, BERT, pre-training, fine-tuning, transfer learning, model distillation, text-to-text, GPT, ethics, cost, decoding strategies, large language models (LLMs), chain-of-thought prompting, reinforcement learning form human feedback (RLHF), scaling laws, chinchilla, multilinguality
:::

```{r tag_DL4NLP, results = 'asis', echo = FALSE, warning = FALSE, message = FALSE}
source("../utils.R")
render_tags("DL4NLP")
```

### Large Language Models {#LLMs}

As also introduced in the above resources, Large Language Models (LLMs) are advanced NLP models that are trained on vast amounts of text data to understand and generate human-like language. They utilize deep learning architectures, particularly transformers, to capture complex linguistic patterns and context. LLMs, such as the GPT and BERT families, can perform a wide range of language tasks, including text generation, translation, summarization, and question answering. Their ability to generalize from large datasets allows them to produce coherent and contextually relevant responses, making them valuable for applications in chatbots, virtual assistants, content creation, and more.

- [Understanding and developing large language models - Stanford CS324 - Page](https://stanford-cs324.github.io/winter2022/)

::: {.callout-note icon=false}
## Further Keywords
capabilities, harms, data, security, legality, modeling, training, architecture, parallelism, scaling laws, adaptation
:::

```{r tag_LLMs, results = 'asis', echo = FALSE, warning = FALSE, message = FALSE}
source("../utils.R")
render_tags("LLMs")
```

**Transformers** are a type of deep learning architecture that has revolutionized natural language processing (NLP) by enabling models to handle sequences of text in parallel and capture long-range dependencies. A central component of their success is the **attention mechanism** [(Vaswani et al., 2017)](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf), which allows the model to capture how each word relates to others in the sequence, improving context understanding beyond sequential processing. Transformers underpin many state-of-the-art NLP models, including BERT, GPT, and T5, and have also been adapted for applications beyond NLP, such as computer vision and reinforcement learning.

- [The Illustrated Transformer - Page](https://jalammar.github.io/illustrated-transformer/)

::: {.callout-note icon=false}
## Further Keywords
encoder, decoder, feed forward neural network, self-attention, tensors, embeddings, query, key, value, softmax, matrix calculation, multi-head attention, positional encoding, residuals, layer-normalization, softmax, loss function
:::

```{r tag_TransformerBlog, results = 'asis', echo = FALSE, warning = FALSE, message = FALSE}
source("../utils.R")
render_tags("TransformerBlog")
```

### Hugging Face {#HuggingFace}

Hugging Face is a popular open-source platform that provides tools and libraries for building, training, and deploying NLP models. It offers a wide range of pre-trained models, datasets, and an easy-to-use interface for fine-tuning models on specific tasks. 

- [NLP in the huggingface ecosystem - Page](https://huggingface.co/course/chapter1/1)

::: {.callout-note icon=false}
## Further Keywords
tranformers, LLMs, datasets tokenizers, fine-tuning pretrained models, building and sharing demos, Python, PyTorch, TensorFlow, pipeline, model hub
:::

```{r tag_HuggingFace, results = 'asis', echo = FALSE, warning = FALSE, message = FALSE}
source("../utils.R")
render_tags("HuggingFace")
```

<!--
[Active learning](00_ml_intro.qmd#AL) can make NLP more efficient by selectively choosing the most informative text examples to label. Instead of annotating entire corpora, the algorithm identifies instances that will most improve the model if labeled, reducing effort and cost while maintaining high performance.

- [Deep Learning with Humans-In-The-Loop: Active Learning for NLP (2025) - BERD Academy Course Information & Registration Page](https://www.berd-nfdi.de/berd-academy/active-learning-2025/)

::: {.callout-note icon=false}
## Further Keywords
human-in-the-loop, deep active learning, AL cycle, implementation, Python 
:::

```{r tag_AL4NLP, results = 'asis', echo = FALSE, warning = FALSE, message = FALSE}
source("../utils.R")
render_tags("AL4NLP")
``` -->

### Label Variation {#Label-Variation}
(Human) label variation [(Plank, 2022)](https://aclanthology.org/2022.emnlp-main.731.pdf) refers to the phenomenon where multiple annotators assign different labels to the same instance. Such variation arises not only from subjective interpretations or ambiguous cases, but also from factors like annotator expertise, background knowledge, and individual biases. In NLP and machine learning, label variation is a critical consideration because standard evaluation metrics often assume a single “correct” or "true" label, whereas human disagreement can reflect multiple valid perspectives or genuine uncertainty. Accounting for this variation, through approaches like probabilistic labels, disagreement-aware learning, or modeling annotator behavior, can improve model robustness and provide more reliable predictions that align with the diversity of human judgments.