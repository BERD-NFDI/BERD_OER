{
  "hash": "4fa6b70f3acd91a6f5e17dcc00558949",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Natural Language Processing\"\n---\n\nNatural Language Processing (NLP) is a field of computer science that focuses on enabling computers to understand, interpret, and generate human language. NLP methods combine linguistics, statistics, and machine learning to analyze text and speech data, enabling tasks such as text classification, sentiment analysis, machine translation, and question answering. Modern NLP often relies on deep learning models, such as transformers, which can capture complex patterns in language and context. By bridging human communication and computational systems, NLP supports applications in information retrieval, virtual assistants, and automated content generation.\n\n- [Introduction to NLP - Stanford CS224N - YouTube Playlist](https://www.youtube.com/watch?v=rmVRLeJRkl4&list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ)\n\n::: {.callout-note icon=false}\n## Further Keywords\nhuman language, word meaning, word2vec algorithm, word2vec objective function gradients, optimization basics, word vectors, PyTorch, dependency parsing, machine translation, question answering, recurrent networks, attention, transformers, distributional semantics, chain rule\n:::\n\n<span class=\"tag tag-data\"># text</span>   <span class=\"tag tag-type\"># Stanford lecture</span> <span class=\"tag tag-type\"># self-paced</span>   <span class=\"tag tag-name\"># NLP with Deep Learning (Stanford CS224N)</span>   <span class=\"tag tag-year\"># 2021</span>   <span class=\"tag tag-material\"># videos</span> \n\nDeep Learning has become a cornerstone of modern NLP, enabling significant advancements in the field. Deep learning models have demonstrated remarkable capabilities in understanding and generating human language. Techniques such as word embeddings, recurrent neural networks (RNNs), and transformers have revolutionized NLP tasks by allowing models to capture semantic relationships, context, and long-range dependencies in text data.\n\n- [Deep Learning for NLP (DL4NLP) - Page](https://slds-lmu.github.io/dl4nlp/)\n\n::: {.callout-note icon=false}\n## Further Keywords\nmachine learning basics, NLP tasks, neural probabilistic language models, word embeddings, deep learning basics, attention, ELMo, tokenization, transformer, encoder, decoder, long sequences, BERT, pre-training, fine-tuning, transfer learning, model distillation, text-to-text, GPT, ethics, cost, decoding strategies, large language models (LLMs), chain-of-thought prompting, reinforcement learning form human feedback (RLHF), scaling laws, chinchilla, multilinguality\n:::\n\n<span class=\"tag tag-data\"># text</span>   <span class=\"tag tag-type\"># LMU lecture</span> <span class=\"tag tag-type\"># self-paced</span>   <span class=\"tag tag-name\"># Deep Learning for NLP</span>   <span class=\"tag tag-year\"># 2022</span>   <span class=\"tag tag-material\"># slides</span> \n\n### Large Language Models {#LLMs}\n\nAs also introduced in the above resources, Large Language Models (LLMs) are advanced NLP models that are trained on vast amounts of text data to understand and generate human-like language. They utilize deep learning architectures, particularly transformers, to capture complex linguistic patterns and context. LLMs, such as the GPT and BERT families, can perform a wide range of language tasks, including text generation, translation, summarization, and question answering. Their ability to generalize from large datasets allows them to produce coherent and contextually relevant responses, making them valuable for applications in chatbots, virtual assistants, content creation, and more.\n\n- [Understanding and developing large language models - Stanford CS324 - Page](https://stanford-cs324.github.io/winter2022/)\n\n::: {.callout-note icon=false}\n## Further Keywords\ncapabilities, harms, data, security, legality, modeling, training, architecture, parallelism, scaling laws, adaptation\n:::\n\n<span class=\"tag tag-data\"># text</span>   <span class=\"tag tag-type\"># Stanford lecture</span>   <span class=\"tag tag-name\"># Understanding and developing large language models (Stanford CS324)</span>   <span class=\"tag tag-year\"># 2022</span>   <span class=\"tag tag-material\"># page</span> \n\n**Transformers** are a type of deep learning architecture that has revolutionized natural language processing (NLP) by enabling models to handle sequences of text in parallel and capture long-range dependencies. A central component of their success is the **attention mechanism** [(Vaswani et al., 2017)](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf), which allows the model to capture how each word relates to others in the sequence, improving context understanding beyond sequential processing. Transformers underpin many state-of-the-art NLP models, including BERT, GPT, and T5, and have also been adapted for applications beyond NLP, such as computer vision and reinforcement learning.\n\n- [The Illustrated Transformer - Page](https://jalammar.github.io/illustrated-transformer/)\n\n::: {.callout-note icon=false}\n## Further Keywords\nencoder, decoder, feed forward neural network, self-attention, tensors, embeddings, query, key, value, softmax, matrix calculation, multi-head attention, positional encoding, residuals, layer-normalization, softmax, loss function\n:::\n\n<span class=\"tag tag-data\"># text</span>   <span class=\"tag tag-type\"># self-paced</span>   <span class=\"tag tag-name\"># The Illustrated Transformer</span>   <span class=\"tag tag-year\"># 2018</span>   <span class=\"tag tag-material\"># page</span> <span class=\"tag tag-material\"># videos</span> \n\n### Hugging Face {#HuggingFace}\n\nHugging Face is a popular open-source platform that provides tools and libraries for building, training, and deploying NLP models. It offers a wide range of pre-trained models, datasets, and an easy-to-use interface for fine-tuning models on specific tasks. \n\n- [NLP in the huggingface ecosystem - Page](https://huggingface.co/course/chapter1/1)\n\n::: {.callout-note icon=false}\n## Further Keywords\ntranformers, LLMs, datasets tokenizers, fine-tuning pretrained models, building and sharing demos, Python, PyTorch, TensorFlow, pipeline, model hub\n:::\n\n<span class=\"tag tag-data\"># text</span>   <span class=\"tag tag-type\"># self-paced</span>   <span class=\"tag tag-name\"># NLP in the huggingface ecosystem</span>   <span class=\"tag tag-year\"># 2025</span>   <span class=\"tag tag-material\"># page</span> <span class=\"tag tag-material\"># videos</span> \n\n<!--\n[Active learning](00_ml_intro.qmd#AL) can make NLP more efficient by selectively choosing the most informative text examples to label. Instead of annotating entire corpora, the algorithm identifies instances that will most improve the model if labeled, reducing effort and cost while maintaining high performance.\n\n- [Deep Learning with Humans-In-The-Loop: Active Learning for NLP (2025) - BERD Academy Course Information & Registration Page](https://www.berd-nfdi.de/berd-academy/active-learning-2025/)\n\n::: {.callout-note icon=false}\n## Further Keywords\nhuman-in-the-loop, deep active learning, AL cycle, implementation, Python \n:::\n\nsource(\"../utils.R\")\nrender_tags(\"AL4NLP\")\n``` -->\n\n### Label Variation {#Label-Variation}\n(Human) label variation [(Plank, 2022)](https://aclanthology.org/2022.emnlp-main.731.pdf) refers to the phenomenon where multiple annotators assign different labels to the same instance. Such variation arises not only from subjective interpretations or ambiguous cases, but also from factors like annotator expertise, background knowledge, and individual biases. In NLP and machine learning, label variation is a critical consideration because standard evaluation metrics often assume a single “correct” or \"true\" label, whereas human disagreement can reflect multiple valid perspectives or genuine uncertainty. Accounting for this variation, through approaches like probabilistic labels, disagreement-aware learning, or modeling annotator behavior, can improve model robustness and provide more reliable predictions that align with the diversity of human judgments.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}