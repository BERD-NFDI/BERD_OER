{
  "hash": "3dff961e7da7db27692e65fba4e78fbd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Trustworthy Machine Learning\"\n---\n\nAs machine learning systems are increasingly used to inform decisions in science, business, and society, trustworthiness becomes a central concern. A trustworthy model is not only accurate but also reliable, transparent, and fair - it provides predictions that users can understand and act upon with confidence. Building such systems involves assessing their **uncertainty**, improving their **explainability**, and ensuring their **interpretability**. These aspects help identify when to trust a modelâ€™s output, when to question it, and how to make machine learning a responsible tool for real-world applications.\n\n### Uncertainty Quantification {#UQ}\n**Uncertainty Quantification (UQ)** involves techniques to measure and express the uncertainty in model predictions. This is crucial in high-stakes applications where decisions based on model outputs can have significant consequences. One key aspect of UQ is distinguishing between different kinds of uncertainty, such as aleatoric and epistemic, which helps practitioners make more informed decisions. More information on this distinction can be found below:\n\n- [Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods - Presentation](https://www.gdsd.statistik.uni-muenchen.de/2021/gdsd_huellermeier.pdf)\n- [Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods - Paper](https://link.springer.com/article/10.1007/s10994-021-05946-3)\n\n::: {.callout-note icon=false}\n## Further Keywords\nsupervised learning, predictive uncertainty, induction principle, learning algorithm, probabilistic predictors, training data, hypothesis space, empirical risk, risk minimizer, sources of uncertainty, Bayes predictor, reducible vs irreducible, ensemble learning, Bayesian agents, mutual information \n:::\n\n   <span class=\"tag tag-type\"># talk</span>   <span class=\"tag tag-name\"># Aleatoric and Epistemic Uncertainty in Machine Learning</span>   <span class=\"tag tag-year\"># 2021</span>   <span class=\"tag tag-material\"># slides</span> <span class=\"tag tag-material\"># paper</span> \n\n\n### Explainability {#XAI}\n\n**Explainable Artificial Intelligence (XAI)** focuses on developing methods that make the behavior of machine learning models understandable to humans. This is particularly important for complex models like deep neural networks, which are often seen as \"black boxes.\" XAI techniques aim to provide insights into how models make decisions, which features are most influential, and how changes in input data affect outputs. This transparency is essential for building trust, ensuring accountability, and facilitating regulatory compliance in AI applications.\n\n- [Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI](https://arxiv.org/abs/1910.10045)\n\n::: {.callout-note icon=false}\n## Further Keywords\npost-hoc explanations, feature importance, saliency maps, decision rules, local vs. global explanations, model transparency\n:::\n\n   <span class=\"tag tag-type\"># paper</span>   <span class=\"tag tag-name\"># Explainable Artificial Intelligence (XAI)</span>   <span class=\"tag tag-year\"># 2019</span>   <span class=\"tag tag-material\"># paper</span> ",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}