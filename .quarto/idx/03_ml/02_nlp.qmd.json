{"title":"Natural Language Processing","markdown":{"yaml":{"title":"Natural Language Processing"},"headingText":"Further Keywords","containsRefs":false,"markdown":"\n\nNatural Language Processing (NLP) is a field of computer science that focuses on enabling computers to understand, interpret, and generate human language. NLP methods combine linguistics, statistics, and machine learning to analyze text and speech data, enabling tasks such as text classification, sentiment analysis, machine translation, and question answering. Modern NLP often relies on deep learning models, such as transformers, which can capture complex patterns in language and context. By bridging human communication and computational systems, NLP supports applications in information retrieval, virtual assistants, and automated content generation.\n\n- [Introduction to NLP - Stanford CS224N - YouTube Playlist](https://www.youtube.com/watch?v=rmVRLeJRkl4&list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ)\n\n::: {.callout-note icon=false}\nhuman language, word meaning, word2vec algorithm, word2vec objective function gradients, optimization basics, word vectors, PyTorch, dependency parsing, machine translation, question answering, recurrent networks, attention, transformers, distributional semantics, chain rule\n:::\n\n```{r tag_IntroNLP, results = 'asis', echo = FALSE, warning = FALSE, message = FALSE}\nsource(\"../utils.R\")\nrender_tags(\"IntroNLP\")\n```\n\nDeep Learning has become a cornerstone of modern NLP, enabling significant advancements in the field. Deep learning models have demonstrated remarkable capabilities in understanding and generating human language. Techniques such as word embeddings, recurrent neural networks (RNNs), and transformers have revolutionized NLP tasks by allowing models to capture semantic relationships, context, and long-range dependencies in text data.\n\n- [Deep Learning for NLP (DL4NLP) - Page](https://slds-lmu.github.io/dl4nlp/)\n\n::: {.callout-note icon=false}\n## Further Keywords\nmachine learning basics, NLP tasks, neural probabilistic language models, word embeddings, deep learning basics, attention, ELMo, tokenization, transformer, encoder, decoder, long sequences, BERT, pre-training, fine-tuning, transfer learning, model distillation, text-to-text, GPT, ethics, cost, decoding strategies, large language models (LLMs), chain-of-thought prompting, reinforcement learning form human feedback (RLHF), scaling laws, chinchilla, multilinguality\n:::\n\n```{r tag_DL4NLP, results = 'asis', echo = FALSE, warning = FALSE, message = FALSE}\nsource(\"../utils.R\")\nrender_tags(\"DL4NLP\")\n```\n\n### Large Language Models {#LLMs}\n\nAs also introduced in the above resources, Large Language Models (LLMs) are advanced NLP models that are trained on vast amounts of text data to understand and generate human-like language. They utilize deep learning architectures, particularly transformers, to capture complex linguistic patterns and context. LLMs, such as the GPT and BERT families, can perform a wide range of language tasks, including text generation, translation, summarization, and question answering. Their ability to generalize from large datasets allows them to produce coherent and contextually relevant responses, making them valuable for applications in chatbots, virtual assistants, content creation, and more.\n\n- [Understanding and developing large language models - Stanford CS324 - Page](https://stanford-cs324.github.io/winter2022/)\n\n::: {.callout-note icon=false}\n## Further Keywords\ncapabilities, harms, data, security, legality, modeling, training, architecture, parallelism, scaling laws, adaptation\n:::\n\n```{r tag_LLMs, results = 'asis', echo = FALSE, warning = FALSE, message = FALSE}\nsource(\"../utils.R\")\nrender_tags(\"LLMs\")\n```\n\n**Transformers** are a type of deep learning architecture that has revolutionized natural language processing (NLP) by enabling models to handle sequences of text in parallel and capture long-range dependencies. A central component of their success is the **attention mechanism** [(Vaswani et al., 2017)](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf), which allows the model to capture how each word relates to others in the sequence, improving context understanding beyond sequential processing. Transformers underpin many state-of-the-art NLP models, including BERT, GPT, and T5, and have also been adapted for applications beyond NLP, such as computer vision and reinforcement learning.\n\n- [The Illustrated Transformer - Page](https://jalammar.github.io/illustrated-transformer/)\n\n::: {.callout-note icon=false}\n## Further Keywords\nencoder, decoder, feed forward neural network, self-attention, tensors, embeddings, query, key, value, softmax, matrix calculation, multi-head attention, positional encoding, residuals, layer-normalization, softmax, loss function\n:::\n\n```{r tag_TransformerBlog, results = 'asis', echo = FALSE, warning = FALSE, message = FALSE}\nsource(\"../utils.R\")\nrender_tags(\"TransformerBlog\")\n```\n\n### Hugging Face {#HuggingFace}\n\nHugging Face is a popular open-source platform that provides tools and libraries for building, training, and deploying NLP models. It offers a wide range of pre-trained models, datasets, and an easy-to-use interface for fine-tuning models on specific tasks. \n\n- [NLP in the huggingface ecosystem - Page](https://huggingface.co/course/chapter1/1)\n\n::: {.callout-note icon=false}\n## Further Keywords\ntranformers, LLMs, datasets tokenizers, fine-tuning pretrained models, building and sharing demos, Python, PyTorch, TensorFlow, pipeline, model hub\n:::\n\n```{r tag_HuggingFace, results = 'asis', echo = FALSE, warning = FALSE, message = FALSE}\nsource(\"../utils.R\")\nrender_tags(\"HuggingFace\")\n```\n\n<!--\n[Active learning](00_ml_intro.qmd#AL) can make NLP more efficient by selectively choosing the most informative text examples to label. Instead of annotating entire corpora, the algorithm identifies instances that will most improve the model if labeled, reducing effort and cost while maintaining high performance.\n\n- [Deep Learning with Humans-In-The-Loop: Active Learning for NLP (2025) - BERD Academy Course Information & Registration Page](https://www.berd-nfdi.de/berd-academy/active-learning-2025/)\n\n::: {.callout-note icon=false}\n## Further Keywords\nhuman-in-the-loop, deep active learning, AL cycle, implementation, Python \n:::\n\n```{r tag_AL4NLP, results = 'asis', echo = FALSE, warning = FALSE, message = FALSE}\nsource(\"../utils.R\")\nrender_tags(\"AL4NLP\")\n``` -->\n\n### Label Variation {#Label-Variation}\n(Human) label variation [(Plank, 2022)](https://aclanthology.org/2022.emnlp-main.731.pdf) refers to the phenomenon where multiple annotators assign different labels to the same instance. Such variation arises not only from subjective interpretations or ambiguous cases, but also from factors like annotator expertise, background knowledge, and individual biases. In NLP and machine learning, label variation is a critical consideration because standard evaluation metrics often assume a single “correct” or \"true\" label, whereas human disagreement can reflect multiple valid perspectives or genuine uncertainty. Accounting for this variation, through approaches like probabilistic labels, disagreement-aware learning, or modeling annotator behavior, can improve model robustness and provide more reliable predictions that align with the diversity of human judgments.","srcMarkdownNoYaml":"\n\nNatural Language Processing (NLP) is a field of computer science that focuses on enabling computers to understand, interpret, and generate human language. NLP methods combine linguistics, statistics, and machine learning to analyze text and speech data, enabling tasks such as text classification, sentiment analysis, machine translation, and question answering. Modern NLP often relies on deep learning models, such as transformers, which can capture complex patterns in language and context. By bridging human communication and computational systems, NLP supports applications in information retrieval, virtual assistants, and automated content generation.\n\n- [Introduction to NLP - Stanford CS224N - YouTube Playlist](https://www.youtube.com/watch?v=rmVRLeJRkl4&list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ)\n\n::: {.callout-note icon=false}\n## Further Keywords\nhuman language, word meaning, word2vec algorithm, word2vec objective function gradients, optimization basics, word vectors, PyTorch, dependency parsing, machine translation, question answering, recurrent networks, attention, transformers, distributional semantics, chain rule\n:::\n\n```{r tag_IntroNLP, results = 'asis', echo = FALSE, warning = FALSE, message = FALSE}\nsource(\"../utils.R\")\nrender_tags(\"IntroNLP\")\n```\n\nDeep Learning has become a cornerstone of modern NLP, enabling significant advancements in the field. Deep learning models have demonstrated remarkable capabilities in understanding and generating human language. Techniques such as word embeddings, recurrent neural networks (RNNs), and transformers have revolutionized NLP tasks by allowing models to capture semantic relationships, context, and long-range dependencies in text data.\n\n- [Deep Learning for NLP (DL4NLP) - Page](https://slds-lmu.github.io/dl4nlp/)\n\n::: {.callout-note icon=false}\n## Further Keywords\nmachine learning basics, NLP tasks, neural probabilistic language models, word embeddings, deep learning basics, attention, ELMo, tokenization, transformer, encoder, decoder, long sequences, BERT, pre-training, fine-tuning, transfer learning, model distillation, text-to-text, GPT, ethics, cost, decoding strategies, large language models (LLMs), chain-of-thought prompting, reinforcement learning form human feedback (RLHF), scaling laws, chinchilla, multilinguality\n:::\n\n```{r tag_DL4NLP, results = 'asis', echo = FALSE, warning = FALSE, message = FALSE}\nsource(\"../utils.R\")\nrender_tags(\"DL4NLP\")\n```\n\n### Large Language Models {#LLMs}\n\nAs also introduced in the above resources, Large Language Models (LLMs) are advanced NLP models that are trained on vast amounts of text data to understand and generate human-like language. They utilize deep learning architectures, particularly transformers, to capture complex linguistic patterns and context. LLMs, such as the GPT and BERT families, can perform a wide range of language tasks, including text generation, translation, summarization, and question answering. Their ability to generalize from large datasets allows them to produce coherent and contextually relevant responses, making them valuable for applications in chatbots, virtual assistants, content creation, and more.\n\n- [Understanding and developing large language models - Stanford CS324 - Page](https://stanford-cs324.github.io/winter2022/)\n\n::: {.callout-note icon=false}\n## Further Keywords\ncapabilities, harms, data, security, legality, modeling, training, architecture, parallelism, scaling laws, adaptation\n:::\n\n```{r tag_LLMs, results = 'asis', echo = FALSE, warning = FALSE, message = FALSE}\nsource(\"../utils.R\")\nrender_tags(\"LLMs\")\n```\n\n**Transformers** are a type of deep learning architecture that has revolutionized natural language processing (NLP) by enabling models to handle sequences of text in parallel and capture long-range dependencies. A central component of their success is the **attention mechanism** [(Vaswani et al., 2017)](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf), which allows the model to capture how each word relates to others in the sequence, improving context understanding beyond sequential processing. Transformers underpin many state-of-the-art NLP models, including BERT, GPT, and T5, and have also been adapted for applications beyond NLP, such as computer vision and reinforcement learning.\n\n- [The Illustrated Transformer - Page](https://jalammar.github.io/illustrated-transformer/)\n\n::: {.callout-note icon=false}\n## Further Keywords\nencoder, decoder, feed forward neural network, self-attention, tensors, embeddings, query, key, value, softmax, matrix calculation, multi-head attention, positional encoding, residuals, layer-normalization, softmax, loss function\n:::\n\n```{r tag_TransformerBlog, results = 'asis', echo = FALSE, warning = FALSE, message = FALSE}\nsource(\"../utils.R\")\nrender_tags(\"TransformerBlog\")\n```\n\n### Hugging Face {#HuggingFace}\n\nHugging Face is a popular open-source platform that provides tools and libraries for building, training, and deploying NLP models. It offers a wide range of pre-trained models, datasets, and an easy-to-use interface for fine-tuning models on specific tasks. \n\n- [NLP in the huggingface ecosystem - Page](https://huggingface.co/course/chapter1/1)\n\n::: {.callout-note icon=false}\n## Further Keywords\ntranformers, LLMs, datasets tokenizers, fine-tuning pretrained models, building and sharing demos, Python, PyTorch, TensorFlow, pipeline, model hub\n:::\n\n```{r tag_HuggingFace, results = 'asis', echo = FALSE, warning = FALSE, message = FALSE}\nsource(\"../utils.R\")\nrender_tags(\"HuggingFace\")\n```\n\n<!--\n[Active learning](00_ml_intro.qmd#AL) can make NLP more efficient by selectively choosing the most informative text examples to label. Instead of annotating entire corpora, the algorithm identifies instances that will most improve the model if labeled, reducing effort and cost while maintaining high performance.\n\n- [Deep Learning with Humans-In-The-Loop: Active Learning for NLP (2025) - BERD Academy Course Information & Registration Page](https://www.berd-nfdi.de/berd-academy/active-learning-2025/)\n\n::: {.callout-note icon=false}\n## Further Keywords\nhuman-in-the-loop, deep active learning, AL cycle, implementation, Python \n:::\n\n```{r tag_AL4NLP, results = 'asis', echo = FALSE, warning = FALSE, message = FALSE}\nsource(\"../utils.R\")\nrender_tags(\"AL4NLP\")\n``` -->\n\n### Label Variation {#Label-Variation}\n(Human) label variation [(Plank, 2022)](https://aclanthology.org/2022.emnlp-main.731.pdf) refers to the phenomenon where multiple annotators assign different labels to the same instance. Such variation arises not only from subjective interpretations or ambiguous cases, but also from factors like annotator expertise, background knowledge, and individual biases. In NLP and machine learning, label variation is a critical consideration because standard evaluation metrics often assume a single “correct” or \"true\" label, whereas human disagreement can reflect multiple valid perspectives or genuine uncertainty. Accounting for this variation, through approaches like probabilistic labels, disagreement-aware learning, or modeling annotator behavior, can improve model robustness and provide more reliable predictions that align with the diversity of human judgments."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"toc-depth":4,"include-after-body":["../_footer.html"],"output-file":"02_nlp.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.31","theme":"cosmo","toc-location":"left","title":"Natural Language Processing"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}