[
  {
    "objectID": "05_reproducibility/03_open.html",
    "href": "05_reproducibility/03_open.html",
    "title": "Open Science",
    "section": "",
    "text": "Open science promotes transparency, accessibility, and collaboration in research by making data, code, and publications freely available. By sharing resources and methods openly, researchers can accelerate knowledge creation, enable reproducibility, and foster a more inclusive and collaborative scientific community.",
    "crumbs": [
      "Research Practices",
      "Open Science"
    ]
  },
  {
    "objectID": "01_data_management/01_wrangling.html",
    "href": "01_data_management/01_wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "Data wrangling is the process of cleaning, transforming, and organizing raw data to prepare it for analysis. How data is processed at this stage directly affects the performance of subsequent analyses, highlighting the importance of careful and deliberate handling to support reliable and meaningful results. Effective wrangling involves standardizing formats, addressing missing or inconsistent values, and structuring data for specific analytical tasks.\n\nWorking with PDFs\nThe Portable Document Format (PDF) is a common file format used for sharing documents. However, extracting data from PDFs can be challenging due to their complex structure. This section covers techniques and tools for converting PDFs into usable data formats.\n\nTurning PDFs into Research Data - GitHub Repository\nTurning PDFs into Research Data (2025) - BERD Academy Information & Registration Page\nTurning PDFs into Research Data (2024) - BERD Academy Information & Registration Page\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\nweb scraping, HTML, Network Inspector Tool, text extraction, OCR, AI for data extraction, NEE, Google Colab\n\n\n# tabular # text # BERD Academy module # Turning PDFs into Research Data # 2024 # 2025 # slides # jupyter notebook",
    "crumbs": [
      "Data Management",
      "Data Wrangling"
    ]
  },
  {
    "objectID": "03_ml/03_trustworthy.html",
    "href": "03_ml/03_trustworthy.html",
    "title": "Trustworthy Machine Learning",
    "section": "",
    "text": "As machine learning systems are increasingly used to inform decisions in science, business, and society, trustworthiness becomes a central concern. A trustworthy model is not only accurate but also reliable, transparent, and fair - it provides predictions that users can understand and act upon with confidence. Building such systems involves assessing their uncertainty, improving their explainability, and ensuring their interpretability. These aspects help identify when to trust a model’s output, when to question it, and how to make machine learning a responsible tool for real-world applications.\n\nUncertainty Quantification\nUncertainty Quantification (UQ) involves techniques to measure and express the uncertainty in model predictions. This is crucial in high-stakes applications where decisions based on model outputs can have significant consequences. One key aspect of UQ is distinguishing between different kinds of uncertainty, such as aleatoric and epistemic, which helps practitioners make more informed decisions. More information on this distinction can be found below:\n\nAleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods - Presentation\nAleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods - Paper\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\nsupervised learning, predictive uncertainty, induction principle, learning algorithm, probabilistic predictors, training data, hypothesis space, empirical risk, risk minimizer, sources of uncertainty, Bayes predictor, reducible vs irreducible, ensemble learning, Bayesian agents, mutual information\n\n\n# talk # Aleatoric and Epistemic Uncertainty in Machine Learning # 2021 # slides # paper\n\n\nExplainability\nExplainable Artificial Intelligence (XAI) focuses on developing methods that make the behavior of machine learning models understandable to humans. This is particularly important for complex models like deep neural networks, which are often seen as “black boxes.” XAI techniques aim to provide insights into how models make decisions, which features are most influential, and how changes in input data affect outputs. This transparency is essential for building trust, ensuring accountability, and facilitating regulatory compliance in AI applications.\n\nExplainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\npost-hoc explanations, feature importance, saliency maps, decision rules, local vs. global explanations, model transparency\n\n\n# paper # Explainable Artificial Intelligence (XAI) # 2019 # paper",
    "crumbs": [
      "Machine Learning",
      "Trustworthy ML"
    ]
  },
  {
    "objectID": "03_ml/00_ml_intro.html",
    "href": "03_ml/00_ml_intro.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Machine Learning (ML) focuses on developing algorithms and models that allow computers to learn patterns from data and make predictions or decisions without being explicitly programmed. It spans a wide range of techniques, from traditional statistical methods to modern deep learning approaches, and is applied in domains such as natural language processing (NLP) and computer vision (CV). ML enables the extraction of insights from large and complex datasets, supporting data-driven decision-making across disciplines.\n\nLearning Paradigms\nMachine learning can be organized into a variety of learning paradigms, which describe different ways in which models interact with data and potential feedback.\nIn Supervised Learning, the model is trained on a labeled dataset, where traditionally each input data point is associated with a corresponding output label. The goal is to learn a mapping from inputs to outputs, enabling the model to make accurate predictions on new, unseen data. Common supervised learning tasks include classification (e.g., spam detection) and regression (e.g., predicting house prices). For a discussion of the assumption of a single ground truth label in the context of NLP, see the section on label variation.\nUnsupervised Learning involves training models on unlabeled data, where the goal is to discover underlying patterns, structures, or relationships within the data. Common unsupervised learning tasks include clustering (e.g., customer segmentation) and dimensionality reduction (e.g., principal component analysis).\nAn open and free introductory course on (supervised) machine learning can be found on the I2ML Course Website from the Statistical Learning and Data Science group at LMU Munich. The course is constructed as self-contained as possible and enables self-study through lecture videos, PDF slides, cheatsheets, quizzes, exercises (with solutions), and notebooks.\n\nI2ML - GitHub Course Page\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\nclassification, k-NN, trees, random forests, bagging, neural networks, hyperparameter tuning, train-validation-test-split, advanced risk minimization, multiclass classification, information theory, curse of dimensionality, regularization, SVM, boosting, Gaussian Processes, imbalanced learning, multitarget learning, online learning, feature selection, sklearn, mlr3\n\n\n# tabular # text # LMU lecture # self-paced # I2ML # 2022 # slides # jupyter notebook # videos\nIn addition to supervised and unsupervised approaches, Self-supervised Learning leverages automatically generated labels from the data itself, allowing models to learn useful representations without requiring costly manual annotation.\n\nSelf-supervised Learning: Generative or Contrastive - Paper\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\nauto-regressive, flow-based, auto-encoding, hybrid generative, mutual information, adverserial, GAN, data augmentation, pretext task, BYOL, SimCLR, MoCo, graph learning\n\n\n# images # text # graphs # self-paced # Self-supervised Learning: Generative or Contrastive # 2021 # paper\n\nReinforcement Learning\nReinforcement Learning is a paradigm where an agent learns to make decisions by interacting with an environment. The agent receives feedback in the form of rewards or penalties based on its actions, and the goal is to learn a policy that maximizes cumulative rewards over time. Reinforcement learning is commonly used in applications such as game playing (e.g., AlphaGo) and robotics.\n\nReinforcement Learning for Business, Economics, and Social Sciences - GitHub Repository\nReinforcement Learning for Business, Economics, and Social Sciences - BERD Academy Information & Registration Page\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\nadaptive experimental designs, bandits, multi-armed, exploration vs. exploitation, regret, reward, stylized data structure, greedy policy, epsilon-greedy, upper confidence bound, uncertainty, Thompson sampling, Bayesian learning, inference with batched bandits\n\n\n# tabular # BERD Academy module # Reinforcement Learning for Business, Economics, and Social Sciences # 2025 # slides\n\n\nActive Learning\nActive Learning (AL) is a machine learning approach that aims to maximize model performance while minimizing the amount of labeled data required. Instead of labeling an entire dataset upfront, the algorithm iteratively identifies the most informative or uncertain examples and queries an expert for labels. This strategy is especially useful in domains where labeling is expensive, time-consuming, or requires specialized knowledge, such as medical diagnosis or linguistic annotation. By focusing effort on the most valuable data points, active learning can significantly improve efficiency and accelerate model training without sacrificing accuracy.\n\n\nAutomated Machine Learning\nAutomated Machine Learning (AutoML) refers to the process of automating the end-to-end workflow of applying machine learning to real-world problems. This includes tasks such as data preprocessing, feature selection, model selection, hyperparameter tuning, and model evaluation. The goal of AutoML is to make machine learning more accessible to non-experts and to improve the efficiency and effectiveness of the model development process.",
    "crumbs": [
      "Machine Learning",
      "Introduction"
    ]
  },
  {
    "objectID": "00_intro/02_berd_nfdi.html",
    "href": "00_intro/02_berd_nfdi.html",
    "title": "Introducing the Organization",
    "section": "",
    "text": "The association Nationale Forschungsdateninfrastruktur (NFDI) is a nationwide initiative in Germany that aims to systematically organize, make accessible, and sustainably preserve research data across disciplines. By providing standards, tools, and services for research data management, the NFDI strengthens open science and ensures that valuable data can be reused in the long term. News and updates about the NFDI initiative can be found on the NFDI website.\nBERD@NFDI is one of the discipline-specific consortia within the NFDI. It focuses on data and research methods from the fields of Business, Economic, and Related Data (BERD). The consortium brings together universities, research institutes, and infrastructure providers to develop community-driven solutions for collecting, managing, and reusing complex and sensitive data in these domains. Further information about BERD@NFDI and its activities is available on the BERD@NFDI website.\n\nBERD Academy\nThe BERD Academy is the training and education hub of BERD@NFDI. It provides resources, courses, and workshops designed to strengthen competencies in research data management, reproducible science, and modern data analysis methods. The Academy serves both early-career researchers and established scholars, promoting best practices and fostering a culture of openness and collaboration in research. On the BERD Academy website, you can find information about upcoming events and explore various training opportunities offered by BERD@NFDI.",
    "crumbs": [
      "Welcome Page",
      "BERD@NFDI"
    ]
  },
  {
    "objectID": "04_code/02_R.html",
    "href": "04_code/02_R.html",
    "title": "R",
    "section": "",
    "text": "R is a programming language and environment designed for statistical computing and data analysis. It provides a wide range of tools for data manipulation, visualization, and modelling, and is widely used in research and applied statistics. Its extensive package ecosystem and reproducible workflow support make R a central tool for implementing and communicating statistical methods.",
    "crumbs": [
      "Computational Tools",
      "R"
    ]
  },
  {
    "objectID": "04_code/01_platforms.html",
    "href": "04_code/01_platforms.html",
    "title": "Platforms",
    "section": "",
    "text": "Platforms for sharing code and data are essential for reproducible and transparent research. They allow researchers to collaborate efficiently, track changes to code and datasets, manage versions, and make their work accessible to the wider community. By centralizing resources, these platforms help reduce errors, facilitate reuse, and ensure that analyses can be validated and extended by others.\n\nGitHub and GitLab\nGitHub and GitLab are widely used platforms for hosting and collaborating on code and data projects. They provide version control through Git, allowing teams to track changes, manage branches, and coordinate contributions. In addition to code management, both platforms support issue tracking, documentation, and continuous integration, making them powerful tools for collaborative and reproducible research workflows.\n\n\nGoogle Colab\nGoogle Colab is an online platform for running Jupyter notebooks in the cloud. It allows researchers to write, execute, and share code without needing to install software locally, providing access to computational resources like GPUs and easy collaboration through shared notebooks. An examplary Jupyter Notebook in Google Colab, using a BERD model, can be found within the course material Turning PDFs into Research Data - for details on the course see the section on Data Wrangling for PDFs.",
    "crumbs": [
      "Computational Tools",
      "Platforms"
    ]
  },
  {
    "objectID": "02_statistics/00_stats_intro.html",
    "href": "02_statistics/00_stats_intro.html",
    "title": "Statistics",
    "section": "",
    "text": "Statistics and Machine Learning are closely connected fields that both aim to learn from data. While statistics traditionally focuses on understanding uncertainty, testing hypotheses, and explaining relationships, machine learning emphasizes prediction, pattern recognition, and handling large, complex datasets. Many machine learning methods build directly on statistical ideas, such as regression, classification, and probability models. At the same time, statistical thinking helps ensure that machine learning results are robust, interpretable, and not driven by random noise. Together, these perspectives form a complementary toolkit for modern data science.\n\nDescriptive Statistics\nDescriptive statistics summarize and present data in a meaningful way. They include measures such as averages, percentages, or visualizations like histograms and boxplots. By condensing large amounts of information into clear numbers and graphics, descriptive statistics help us understand the main patterns and characteristics of data.\n\n\nStatistical Inference\nWhile descriptives describe the data at hand, statistical inference goes a step further: it allows us to draw conclusions about larger populations based on samples. Methods such as confidence intervals and hypothesis tests help us quantify uncertainty and assess whether observed patterns are likely due to chance or reflect underlying relationships.\n\n\nRegression Analysis\nRegression is a powerful tool for studying relationships between variables. It can be used to explain outcomes, make predictions, and identify important influencing factors. From simple linear regression to more complex models, regression provides a framework for quantifying associations and controlling for multiple variables at once.\n\n\nExperimental Design\nWell-designed experiments are crucial for drawing reliable conclusions. By carefully planning how data are collected - for example, through randomization, control groups, and replication - researchers can reduce bias and increase the validity of their findings. Good experimental design ensures that causal effects can be identified, not just correlations.\n\n\nCommon Pitfalls in Statistical Analysis\nStatistical analyses are powerful, but they can also be misused. Common pitfalls include confusing correlation with causation, overinterpreting small sample sizes, p-hacking, or ignoring the assumptions behind statistical methods. Recognizing these challenges is an important step toward conducting and interpreting research responsibly.\n\n\nOfficial Statistics\n\nOfficial statistics are produced by government agencies and international organizations to provide reliable, standardized data on various aspects of society, economy, and environment. They play a key role in informing public policy, research, and public understanding. Examples include census data, labor market statistics, and health indicators.\n\nMicrosimulation & Machine Learning with Official Statistics Data - GitHub Repository\nMicrosimulation & Machine Learning with Official Statistics Data - BERD Academy Information & Registration Page\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\nmicrosimulation, official statistics, static ageing techniques, dynamic ageing, MikroSim, Big Data, primary data, secondary data, consumer price index, web based statistics, social media, signal vs. noise, feature selection, Bayesian approach, mobile network data, Google trends, information extraction, parallization, sensor data, eXplainable AI\n\n\n# BERD Academy module # Microsimulation & Machine Learning with Official Statistics Data # 2023 # slides # quarto markdown",
    "crumbs": [
      "Statistics",
      "Introduction"
    ]
  },
  {
    "objectID": "02_statistics/01_descriptive.html",
    "href": "02_statistics/01_descriptive.html",
    "title": "Descriptive",
    "section": "",
    "text": "Tags:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Welcome to our open educational resources (OER) hub! This site brings together materials from various sources to support learning, teaching, and exploring data-driven methods. Whether you’re joining for the first time or coming back to revisit a topic, we’re glad you’re here!",
    "crumbs": [
      "Welcome Page",
      "How To"
    ]
  },
  {
    "objectID": "index.html#how-to",
    "href": "index.html#how-to",
    "title": "Welcome!",
    "section": "How To",
    "text": "How To\nThis site is designed for easy navigation across a growing collection of learning materials. You can browse by topic, explore materials from specific events, or filter content by data type or format. Whether you’re preparing for a workshop, reviewing past content, or looking for examples and code, this site has you covered.\nAll resources on this site are tagged with keywords indicating, if applicable, the event they belong to, the data type they cover, and the material format. Use these tags to filter or combine search queries. You can also use the search bar at the top right to find specific topics or keywords.\nIf you’re looking to go beyond self-study, join our interactive learning sessions! Discover our upcoming workshops and events hosted by the BERD Academy, or learn more about the BERD@NFDI consortium and the NFDI initiative.\n\nSearching for content?\nNot sure what you’re looking for yet? Below, you’ll find a list of events, data types, and material types available on this site. Use these tags to browse or to guide your search.\n\nData Types\n# visual # network # mobile # tabular # text # images # graphs # audio\nData Types Overview\n\n\nResource Types\n# paper # BERD Academy module # LMU lecture # self-paced # Stanford lecture # TUM lecture # talk # MIT lecture # Python library\n\n\nMaterial Types\n# paper # slides # R script # jupyter notebook # quarto markdown # videos # page # GitHub repository",
    "crumbs": [
      "Welcome Page",
      "How To"
    ]
  },
  {
    "objectID": "index.html#tags-overview",
    "href": "index.html#tags-overview",
    "title": "Welcome!",
    "section": "Tags Overview",
    "text": "Tags Overview",
    "crumbs": [
      "Welcome Page",
      "How To"
    ]
  },
  {
    "objectID": "04_code/03_python.html",
    "href": "04_code/03_python.html",
    "title": "Python",
    "section": "",
    "text": "Python is a high-level, general-purpose programming language widely used in data analysis, scientific computing, and software development. Its readability, flexibility, and extensive ecosystem make it a standard tool for implementing computational methods and workflows across disciplines.",
    "crumbs": [
      "Computational Tools",
      "Python"
    ]
  },
  {
    "objectID": "04_code/00_tools.html",
    "href": "04_code/00_tools.html",
    "title": "Tools for Software & Data Management",
    "section": "",
    "text": "Efficient management of software, code, and research data is essential for reliable and reproducible research. Using the right tools helps organize workflows, document analyses, track changes, and share data effectively, making research more transparent, collaborative, and easier to validate.\n\nStatistical Practice\nApplying sound statistical workflows ensures that analyses are accurate, interpretable, and reproducible. These practices, including effective data handling, visualization, and leveraging coding tools, can be further developed in the BERD Academy course on statistical practice.\n\n\nAI Tools in Research\nA variety of AI tools are available to support research activities, including data analysis, visualization, and interpretation. These tools, many of which are generative as LLMs, can help researchers to automate repetitive tasks, identify patterns in data, and generate new insights.\n\nHow to get the best of AI tools in Research - GitHub Repository\nHow to get the best of AI tools in Research - BERD Academy Information & Registration Page\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\nLarge Language Models (LLMs), Generative Artificial Intelligence (AI), prompting techniques, literature review & management, data collection & analysis, communicating results, ethical considerations, semantic tools, visual search tools, data protection, biases\n\n\n# text # BERD Academy module # How to get the best of AI tools in Research # 2025 # slides # GitHub repository",
    "crumbs": [
      "Computational Tools",
      "Tools"
    ]
  },
  {
    "objectID": "03_ml/02_nlp.html",
    "href": "03_ml/02_nlp.html",
    "title": "Natural Language Processing",
    "section": "",
    "text": "Natural Language Processing (NLP) is a field of computer science that focuses on enabling computers to understand, interpret, and generate human language. NLP methods combine linguistics, statistics, and machine learning to analyze text and speech data, enabling tasks such as text classification, sentiment analysis, machine translation, and question answering. Modern NLP often relies on deep learning models, such as transformers, which can capture complex patterns in language and context. By bridging human communication and computational systems, NLP supports applications in information retrieval, virtual assistants, and automated content generation.\n\nIntroduction to NLP - Stanford CS224N - YouTube Playlist\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\nhuman language, word meaning, word2vec algorithm, word2vec objective function gradients, optimization basics, word vectors, PyTorch, dependency parsing, machine translation, question answering, recurrent networks, attention, transformers, distributional semantics, chain rule\n\n\n# text # Stanford lecture # self-paced # NLP with Deep Learning (Stanford CS224N) # 2021 # videos\nDeep Learning has become a cornerstone of modern NLP, enabling significant advancements in the field. Deep learning models have demonstrated remarkable capabilities in understanding and generating human language. Techniques such as word embeddings, recurrent neural networks (RNNs), and transformers have revolutionized NLP tasks by allowing models to capture semantic relationships, context, and long-range dependencies in text data.\n\nDeep Learning for NLP (DL4NLP) - Page\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\nmachine learning basics, NLP tasks, neural probabilistic language models, word embeddings, deep learning basics, attention, ELMo, tokenization, transformer, encoder, decoder, long sequences, BERT, pre-training, fine-tuning, transfer learning, model distillation, text-to-text, GPT, ethics, cost, decoding strategies, large language models (LLMs), chain-of-thought prompting, reinforcement learning form human feedback (RLHF), scaling laws, chinchilla, multilinguality\n\n\n# text # LMU lecture # self-paced # Deep Learning for NLP # 2022 # slides\n\nLarge Language Models\nAs also introduced in the above resources, Large Language Models (LLMs) are advanced NLP models that are trained on vast amounts of text data to understand and generate human-like language. They utilize deep learning architectures, particularly transformers, to capture complex linguistic patterns and context. LLMs, such as the GPT and BERT families, can perform a wide range of language tasks, including text generation, translation, summarization, and question answering. Their ability to generalize from large datasets allows them to produce coherent and contextually relevant responses, making them valuable for applications in chatbots, virtual assistants, content creation, and more.\n\nUnderstanding and developing large language models - Stanford CS324 - Page\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\ncapabilities, harms, data, security, legality, modeling, training, architecture, parallelism, scaling laws, adaptation\n\n\n# text # Stanford lecture # Understanding and developing large language models (Stanford CS324) # 2022 # page\nTransformers are a type of deep learning architecture that has revolutionized natural language processing (NLP) by enabling models to handle sequences of text in parallel and capture long-range dependencies. A central component of their success is the attention mechanism (Vaswani et al., 2017), which allows the model to capture how each word relates to others in the sequence, improving context understanding beyond sequential processing. Transformers underpin many state-of-the-art NLP models, including BERT, GPT, and T5, and have also been adapted for applications beyond NLP, such as computer vision and reinforcement learning.\n\nThe Illustrated Transformer - Page\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\nencoder, decoder, feed forward neural network, self-attention, tensors, embeddings, query, key, value, softmax, matrix calculation, multi-head attention, positional encoding, residuals, layer-normalization, softmax, loss function\n\n\n# text # self-paced # The Illustrated Transformer # 2018 # page # videos\n\n\nHugging Face\nHugging Face is a popular open-source platform that provides tools and libraries for building, training, and deploying NLP models. It offers a wide range of pre-trained models, datasets, and an easy-to-use interface for fine-tuning models on specific tasks.\n\nNLP in the huggingface ecosystem - Page\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\ntranformers, LLMs, datasets tokenizers, fine-tuning pretrained models, building and sharing demos, Python, PyTorch, TensorFlow, pipeline, model hub\n\n\n# text # self-paced # NLP in the huggingface ecosystem # 2025 # page # videos\n\n\n\nLabel Variation\n(Human) label variation (Plank, 2022) refers to the phenomenon where multiple annotators assign different labels to the same instance. Such variation arises not only from subjective interpretations or ambiguous cases, but also from factors like annotator expertise, background knowledge, and individual biases. In NLP and machine learning, label variation is a critical consideration because standard evaluation metrics often assume a single “correct” or “true” label, whereas human disagreement can reflect multiple valid perspectives or genuine uncertainty. Accounting for this variation, through approaches like probabilistic labels, disagreement-aware learning, or modeling annotator behavior, can improve model robustness and provide more reliable predictions that align with the diversity of human judgments.",
    "crumbs": [
      "Machine Learning",
      "NLP"
    ]
  },
  {
    "objectID": "03_ml/01_dl.html",
    "href": "03_ml/01_dl.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Deep learning (DL) is a specialised subfield of machine learning that employs neural networks to represent and approximate complex, non-linear relationships in data. DL methods have achieved leading empirical performance on a range of tasks, including pattern recognition, natural language processing, and data generation, and have contributed substantially to recent methodological advances.\n\nIntroduction\nAn introductory course on deep learning is offered by Stanford University and encompasses an introduction to the fundamentals of deep learning, as well as examples of more sophisticated topics, including deep reinforcement learning.\n\nDeep Learning - Stanford CS231N - YouTube Playlist\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\nconvolutional neural networks (CNN), image classification, loss functions, optimization, training, software, recurrent neural networks (RNN), detection and segmentation, computer vision, generative models, hardware, adverserial training\n\n\n# images # Stanford lecture # self-paced # Introduction to Deep Learning (Stanford CS231N) # 2017 # videos\nAnother introductory course is offered by TUM, providing an overview of deep learning along with a detailed look at model training, optimization methods, and the use of non-linear layers in neural networks.\n\nIntroduction to Deep Learning (I2DL) - Slides\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\nML basics, linear regression, maximum likelihood, neural networks, computational graphs, optimization, backpropagation, scaling optimization, stochastic gradient descent (SGD), training, CNNs, RNN, transformers, advanced DL\n\n\n# images # text # audio # TUM lecture # I2DL # 2025 # slides\n\n\nMethods\nDeep learning methods are built around the concept of artificial neural networks, which are trained using optimization techniques such as gradient descent. Core components include loss functions, which measure the difference between predictions and intended outcomes, and optimization algorithms, such as stochastic gradient descent (SGD) and Adam, which update network parameters. Regularization techniques, including dropout and weight decay, help prevent overfitting, while training strategies, such as batch normalization and learning rate schedules, support more efficient and robust learning.\nHyperparameter tuning is an essential part of training deep learning models efficiently. For practical guidance, see:\n\nTune: Scalable Hyperparameter Tuning - Slides\nTune: Scalable Hyperparameter Tuning - Code Examples\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\nray tune, search space, search algorithm, scheduler, tuner, trials, function API, class API, random search, grid search, Bayesian optimization, Bandit optimization, tree-parzen estimators, gradient-free optimization, optuna search algorithms, median stopping rule, ASHA, Population Based Training (PBT)\n\n\n# self-paced # Tune: Scalable Hyperparameter Tuning # 2025 # slides\nUnderstanding and applying effective training strategies, including learning rate schedules and batch normalization, is crucial for model performance. For hands-on examples using PyTorch, see:\n\nGuide to Pytorch Learning Rate Scheduling\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\nlambda, multiplicative, step, MultiStep, exponential, cosine annealing, cyclic, OneCycle, cosine annealing with warm restarts\n\n\n# self-paced # Guide to Pytorch Learning Rate Scheduling # 2020 # jupyter notebook\nFor a broader overview of deep learning in Python, including an introduction to PyTorch with tutorials and practical exercises, see the Deep Learning with Python coding section.\n\n\nArchitectures\nDeep learning architectures define how neural networks are structured to process different types of data. Convolutional Neural Networks (CNNs) are specialized for spatial data such as images, while Recurrent Neural Networks (RNNs) handle sequential data, including text and time series. Transformers use self-attention mechanisms to model long-range dependencies in sequences and represent the current state-of-the-art for many NLP and sequence tasks. Autoencoders are unsupervised models commonly used for dimensionality reduction, learning meaningful representations, and detecting anomalies.\nThis application provides a technical demo of a CNN using the MNIST dataset, a widely used benchmark of handwritten digits. Users can draw their own digits and observe how the network processes them. The source code and related publication are also available:\n\nAn Interactive Node-Link Visualization of Convolutional Neural Networks\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\nCNN, MNIST, visualization, filters, feature maps, 3D fully-connected network, 2D, nodes, bottom layer, hidden layer, output layer, convolutional layer, flattening\n\n\n# images # self-paced # An Interactive Node-Link Visualization of Convolutional Neural Networks # 2015 # paper # slides # videos\nThe following lecture-style video covers the Transformer architecture and its applications and is best for those familiar with neural networks and embeddings.\n\nIntroduction to the Transformer Architecture - YouTube Video\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\ncomputer vision, NLP, reinforcement learning, speech, translation, graphs, attention, tokenization, embeddings, positional encoding, multi-headed self-attention, point-wise MLP, GeLU, layer normalization, encoder, decoder, masked self-attention, generation, cross attention, feedforward, compute budget heuristics, ExaFLOPs, GPU, mixture of experts, ViT, convolution-augmented transformer, decision transformer\n\n\n# images # text # audio # self-paced # Introduction to the Transformer Architecture # 2022 # slides # videos\nMore on Transformers can be found in the NLP chapter.\n\n\nModel Classes\nBeyond general architectures, deep learning includes several specialized model classes that address specific goals. Generative Adversarial Networks (GANs) learn to generate new data by creating a competition between a generator and a discriminator. Variational Autoencoders (VAEs) extend standard autoencoders probabilistically, modeling a latent variable distribution and optimizing a variational lower bound to enable generative modeling. Recently, diffusion models have gained popularity as generative models that iteratively refine noisy data into structured outputs.\nFor a deeper dive into diffusion models, see:\n\nWhat are Diffusion Models?\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\nMarkov chain, random noise, latent variable, Gaussian noise, forward process, Langevin dynamics, reverse process, Bayes’ rule, parametrization, training loss, noise-conditioned score networks (NCSN), classifier guided diffusion, classifier-free guidance, sampling steps, progressive distillation, DDPM, DDIM, consistency models, latent variable space, CLIP, U-Net, Transformer, ControlNet\n\n\n# images # self-paced # What are Diffusion Models? # 2025 # page\nLaplace Redux provides a practical library for applying Laplace approximations in neural networks, whether for entire networks, subnetworks, or just the last layer. The package enables posterior approximations, marginal-likelihood estimation, and posterior predictive computations, and includes multiple example scenarios. Implementing Laplace approximations from scratch is difficult due to the Hessian computations, so this library offers a straightforward way to experiment with these techniques in code.\n\nLaplace Redux - Effortless Bayesian Deep Learning\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\ntorch, marginal likelihood, laplace on LLMs, serialization, backend, regression, calibration, GP inference, huggingface LLMs, reward modeling, API\n\n\n# self-paced # Laplace Redux - Effortless Bayesian Deep Learning # 2025 # slides",
    "crumbs": [
      "Machine Learning",
      "Deep Learning"
    ]
  },
  {
    "objectID": "01_data_management/00_data_intro.html",
    "href": "01_data_management/00_data_intro.html",
    "title": "Overview of Data Types",
    "section": "",
    "text": "Data is a foundation for understanding complex systems, making informed decisions, and driving innovation. Recognizing the type of data you’re working with is crucial. It determines how you can summarize it, visualize it, and extract meaningful insights. By thinking carefully about data types, we can avoid misleading conclusions, choose the right analytical tools, and unlock the full potential of the information at hand.\n\nTabular\nTabular data is structured in rows and columns, much like a spreadsheet or a database table. Each row typically represents an observation, and each column represents a variable or feature. This format is one of the most common in data analysis and is used in tasks like regression, classification, and exploratory data analysis. Examples include survey results, financial records, and experiment logs.\n\n\nText\nText data includes any data in written language, ranging from short phrases to full documents. It can come from diverse sources such as interviews, survey responses, articles, or social media posts. In some cases, text may need to be extracted from other formats, such as PDFs, with more information on data wrangling available in the section on PDFs. Analyzing text often involves preprocessing steps like tokenization, and tasks may include sentiment analysis, topic modeling, or text classification, for which more detail can be found on the NLP page.\n\n\nVisual\nVisual data includes images, videos, and other forms of visual representations that convey information through patterns, shapes, or spatial structure. This can encompass a wide range of content such as medical imagery, satellite views, or rendered scenes. Analyzing visual data often involves tasks like classification, segmentation, or object detection, using approaches from computer vision and image analysis.\n\nCommon Limitations of Image Processing Metrics: A Picture Story\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\nimage-level classification, semantic segmentation, object detection, instance segmentation, pitfalls, counting metrics, multi-threshold metrics, distance-based metrics, MCC, Cohen’s Kappa, sensitivity, specificity, AUROC, positive predictive value, DSC, IoU, Hausdorff distance, calibration metrics, FROC, mismatch, high class imbalance\n\n\n# visual # paper # Common Limitations of Image Processing Metrics: A Picture Story # 2023 # paper\n\n\nTime Series\nTime series data consists of observations recorded at regular (or irregular) time intervals. It captures how variables evolve over time, making it particularly useful in domains like finance, health monitoring, environmental science, or sensor data. Key methods include forecasting, anomaly detection, and trend analysis.\n\n\nNetwork Data\nNetwork data represents relationships or interactions between entities, often visualized as graphs with nodes (entities) and edges (connections). This type of data is prevalent in social networks, transportation systems, biological networks, and communication networks. Analyzing network data involves techniques like centrality measures, network visualization and modeling of the network.\n\nA Connected World: Data Analysis for Real-World Network Data - GitHub Repository\nA Connected World: Data Analysis for Real-World Network Data - BERD Academy Information & Registration Page\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\nvisualization of networks, Fruchterman-Reingold algorithm, graph density, node degree, degree centrality, modelling networks, exponential random graph models (ERGM), latent variable models (LVM)\n\n\n# network # BERD Academy module # A Connected World: Data Analysis for Real-World Network Data # 2023 # slides # R script\n\n\nMobile Phone Data\nMobile phone data encompasses information collected from mobile devices, including call records, text messages, app usage, and location data. This type of data is often used in studies related to human behavior, social networks, and mobility patterns. Analyzing mobile phone data requires careful consideration of privacy and ethical issues.\n\nData Challenge: Mobile Phone Data - GitHub Repository\nData Challenge: Mobile Phone Data - BERD Academy Information & Registration Page\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\nmobile network data, official statistics, gravity models, anonymization, commuting patterns, public transport planning, legal framework, infrastructure quality, welfare effects\n\n\n# mobile # BERD Academy module # Data Challenge: Mobile Phone Data # 2024 # slides\nOnce we recognize the type of data we’re working with, the next step is data wrangling — preparing and transforming data so it’s ready for analysis.",
    "crumbs": [
      "Data Management",
      "Data Types"
    ]
  },
  {
    "objectID": "05_reproducibility/00_repr_intro.html",
    "href": "05_reproducibility/00_repr_intro.html",
    "title": "Reproducibility",
    "section": "",
    "text": "Reproducibility ensures that research results can be independently verified and trusted. By carefully documenting methods, managing code and data, and using consistent workflows, researchers make it possible for others, or themselves in the future, to replicate analyses and validate findings.\n\nMake your research reproducible - a hands-on course - GitHub Repository\nMake your research reproducible - a hands-on course - BERD Academy Information & Registration Page\n\n\n\n\n\n\n\nFurther Keywords\n\n\n\nFAIR and reproducible research, team projects, organization, version control with Git, stable computing environments, automation, publication platforms, licensing, open science, R, Python, publishing\n\n\n# BERD Academy module # Make your research reproducible – a hands-on course # 2023 # 2024 # 2025 # GitHub repository # quarto markdown",
    "crumbs": [
      "Research Practices",
      "Reproducibility"
    ]
  }
]